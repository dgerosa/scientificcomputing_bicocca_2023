{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c7d7a1",
   "metadata": {},
   "source": [
    "# Go faster: numba and multiprocessing\n",
    "\n",
    "*Davide Gerosa (Milano-Bicocca)*\n",
    "\n",
    "**Sources**: The data frog: https://thedatafrog.com/en/articles/make-python-fast-numba \n",
    "\n",
    "**Sources**: my own research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc5596",
   "metadata": {},
   "source": [
    "The usual criticism to python is that it's slow. The underlying reason is that python is an interpreted language and not a compiled language (but that's also why python it's nice and simple!).\n",
    "\n",
    "Numpy largely solves the speed problem in most cases, but not everything can be written in a numpy-compatible form (or even if it can, you might need to rewrite substantial chunks of code). \n",
    "\n",
    "Today we look into two complementary strategies to speed up python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97c54a",
   "metadata": {},
   "source": [
    "# Part 1. Just-in-time compilation: numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44029555",
   "metadata": {},
   "source": [
    "## Extensions \n",
    "\n",
    "There are a few ways to make python faster, which are called _extensions_. Typically, these boils down to either\n",
    "- Write some core functions in compiled language like C or Fortran and interface it with python. This is also useful to recycle some legacy code written by the supervisor of your supervisor who doesn't know python.\n",
    "- Use a library that converts python into compiled code (with some restrictions)\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be80469",
   "metadata": {},
   "source": [
    "* **C**\n",
    "  * [C-API](https://docs.python.org/3/c-api/index.html) : the\n",
    "    standard python interpreter (cpython) is written in C, so it is\n",
    "    natural that we can write C code to interact with our python code.\n",
    "    This is the python C-API.  Since numpy is also written in C, we\n",
    "    can work with numpy arrays in C code as well.\n",
    "    This will give us the performance of C compiled code, but the\n",
    "    downside is that we lose a lot of what makes python great.  We\n",
    "    need to pass data into C as pointers and cast them into types that\n",
    "    represent the arrays we use.  This means writing a lot of\n",
    "    boilerplate code just to deal with some simple operations. These days, there are better methods for most applications.\n",
    "  * [ctypes](https://docs.python.org/3/library/ctypes.html) : this\n",
    "    is a module that allows you to call functions in shared libraries.\n",
    "    This is part of standard python. With ctypes, you don't need to modify your C code -- you just need to\n",
    "    define an interface to the C function in python.  However, the calling\n",
    "    mechanism can be slow. There is support for numpy through numpy.ctypeslib.\n",
    "\n",
    "* **Fortran**\n",
    "  * [f2py](https://numpy.org/doc/stable/f2py/) : this is part of\n",
    "    numpy.  It allows for easy calling of Fortran from python.\n",
    "    You essentially just need to add some comments to your Fortran\n",
    "    code to allow f2py to build an interface.  f2py understands the\n",
    "    different orderings of indices between C and Fortran arrays.\n",
    "\n",
    "* **python**\n",
    "  * [Cython](https://cython.org/) : this is a superset of python that can convert python into\n",
    "    compiled C code. The advantage here is that the code looks like python, with some\n",
    "    declarations of the variable types with `cdef`.  Performance can be\n",
    "    really great when you need to explicitly write out loops over\n",
    "    numpy array indices. Advantage: you can identify the bottleneck of your code and cytonize only that part. \n",
    "  * [Numba](https://numba.pydata.org/) : this is a just-in-time\n",
    "    compiler.  It just requires a simple decorator and then it will\n",
    "    compile a python function the first time it is encountered.\n",
    "\n",
    "* **GPU**: The modern (think deep learning) way of going faster is actually changing hardware and using GPUs instead of CPUs. That could probably fill an entire class, but [this](https://thedatafrog.com/en/articles/boost-python-gpu/) is a nice tutorial. There's even [some hardware](https://cloud.google.com/tpu/docs/intro-to-tpu) that is specifically built for AI (Google builds it to run TensorFlow).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0809b",
   "metadata": {},
   "source": [
    "Just-in-time compilation seems to be the cool thing to do these dats, so here we'll look at Numba. But before that... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a337719",
   "metadata": {},
   "source": [
    "## Python decorators\n",
    "\n",
    "With numba, the compilation of a python function is triggered by a decorator.\n",
    "\n",
    "A python decorator is a function that takes another function as input, modifies it, and returns the modified function to the user. I realize that this sentence sounds tricky, but it's not. \n",
    "\n",
    "```\n",
    "def decorator(function):\n",
    "    [do something]\n",
    "    output=function(input)\n",
    "    [do something else]\n",
    "    return [result]\n",
    "```\n",
    "\n",
    "Then if you have any function\n",
    "\n",
    "```\n",
    "def myprecious(ring):\n",
    "    return gollum\n",
    "```       \n",
    " \n",
    "You can redefine it to e.g.\n",
    "\n",
    "```\n",
    "myprecious=decorator(myprecious)\n",
    "```\n",
    "\n",
    "or equivalently add\n",
    "\n",
    "```\n",
    "@decorator\n",
    "def myprecious(ring):\n",
    "    return gollum\n",
    "```   \n",
    "\n",
    "before the function call.\n",
    "    \n",
    "\n",
    "\n",
    "Remember that in python everything is an object. Functions are objects, and classes are objects too. For instance, let's take this simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38aba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello():\n",
    "  print('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd3f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215921d1",
   "metadata": {},
   "source": [
    "`hello` is a function object, so we can pass it to another function like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe340f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sure(func):\n",
    "  def wrapper():\n",
    "    while 1:\n",
    "      res = input('are you sure you want to greet the world? [y/n]')\n",
    "      if res=='n':\n",
    "        return\n",
    "      elif res=='y':\n",
    "        o = func()\n",
    "        return o\n",
    "  return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34fd58",
   "metadata": {},
   "source": [
    "This is a decorator! `make_sure` takes an input function and returns a new function that wraps the input function.\n",
    "\n",
    "Below, we decorate the function `hello`, and `whello` is the decorated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0806aae1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "60529bfd-6040-4108-8631-751903723148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you sure you want to greet the world? [y/n]\n",
      "are you sure you want to greet the world? [y/n]\n",
      "are you sure you want to greet the world? [y/n]\n",
      "are you sure you want to greet the world? [y/n]n\n"
     ]
    }
   ],
   "source": [
    "whello = make_sure(hello)\n",
    "whello()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ff6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_sure\n",
    "def justtwo():\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f754b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you sure you want to greet the world? [y/n]y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justtwo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc91925",
   "metadata": {},
   "source": [
    "Of course, we can use the make_sure decorator on any function that has the same signature as `func` (can work without arguments, and no need to retrieve the output).\n",
    "\n",
    "We know enough about decorators to use numba. Just one word about the syntax: we can also decorate a function in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2194919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "4cbf3e70-3e72-4283-fc4c-3b4226c31cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you sure you want to greet the world? [y/n]n\n"
     ]
    }
   ],
   "source": [
    "@make_sure\n",
    "def hello():\n",
    "  print('Hello world')\n",
    "\n",
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef34cd8",
   "metadata": {},
   "source": [
    "There is really nothing mysterious about this, it's just a nice and easy syntax to decorate the function as soon as you write it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a1ba3",
   "metadata": {},
   "source": [
    "## Just-in-time compilation with numba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cdbf2",
   "metadata": {},
   "source": [
    "Numba is able to compile python code into bytecode optimized for your machine, with the help the LLVM library. You don't really need to know what LLVM is to follow this tutorial, but here is a [nice introduction to LLVM](https://www.infoworld.com/article/3247799/what-is-llvm-the-power-behind-swift-rust-clang-and-more.html) in case you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf2c87",
   "metadata": {},
   "source": [
    "Here is a function that can take a bit of time. This function takes a list of numbers, and returns the standard deviation of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b44b0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def std(xs):\n",
    "  # compute the mean\n",
    "  mean = 0\n",
    "  for x in xs:\n",
    "    mean += x\n",
    "  mean /= len(xs)\n",
    "  # compute the variance\n",
    "  ms = 0\n",
    "  for x in xs:\n",
    "     ms += (x-mean)**2\n",
    "  variance = ms / len(xs)\n",
    "  std = math.sqrt(variance)\n",
    "  return std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42137480",
   "metadata": {},
   "source": [
    "As we can see in the code, we need to loop twice on the sample of numbers: first to compute the mean, and then to compute the variance, which is the square of the standard deviation.\n",
    "\n",
    "Obviously, the more numbers in the sample, the more time the function will take to complete. Let's start with 10 million numbers, drawn from a Gaussian distribution of unit standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ccda33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.normal(0, 1, 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fea6d324",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "f9b0443c-c07f-4f27-df97-e425d5733f52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999433547917166"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ceba18",
   "metadata": {},
   "source": [
    "The function takes a second or so to compute the standard deviation of the sample.\n",
    "\n",
    "Now, let's import the njit decorator from numba, and decorate our std function to create a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66874867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "c_std = njit(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "394942f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "06a6ccaa-f527-4144-87f6-af98ae5d0343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999433547917166"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_std(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7c65f",
   "metadata": {},
   "source": [
    "The performance improvement might not seem striking, but that's because the first time the function is called, numba will need to compile the function, which takes a bit of time.\n",
    "\n",
    "But we can quantify the improvement using the timeit magic function, first for the interpreted version of the std function, and then for the compiled version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd31adb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "18f32748-fcf9-46db-8e18-f3229874fee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s ± 39.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit std(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9173ebec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "da394aff-ff2b-429f-aa90-fcd902d96377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms ± 30.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit c_std(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72aeb1a",
   "metadata": {},
   "source": [
    "The compiled function is 100 times faster!\n",
    "\n",
    "But obviously we did not have to go into such trouble to compute the standard deviation of our array. For that, we can simply use numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c84959b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "68912967-c99d-4b23-fbcd-070388619666"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997743508708942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c2db3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "386d16b4-8148-4b9d-ff8f-e50f240fbf03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ms ± 357 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d056d",
   "metadata": {},
   "source": [
    "In this particular case numba and numpy are comparable, but we'll see numba is much more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3962f",
   "metadata": {},
   "source": [
    "## Calculation of $\\pi$\n",
    "\n",
    "The number $\\pi$ can be estimated with a very elegant Monte Carlo method.\n",
    "\n",
    "Just consider a square of side L=2, centred on (0,0). In this square, we fit a circle of radius R=1, as shown in the figure below.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cbernet/maldives/master/numba/pi_mc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44eeb59",
   "metadata": {},
   "source": [
    "\n",
    "The ratio of the circle area to the square area is\n",
    "\n",
    "$$r = \\frac{A_c}{A_s} = \\frac{\\pi R^2}{L^2} = \\pi / 4$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\pi = 4 r$$\n",
    "\n",
    "So if we can estimate this ratio, we can estimate pi!\n",
    "\n",
    "And to estimate this ratio, we will simply shoot a large number of points in the square, following a uniform probability distribution. The fraction of the points falling in the circle is an estimator of r.\n",
    "\n",
    "Obviously, the more points, the more precise this estimator will be, and the more decimals of pi can be computed.\n",
    "\n",
    "Let's implement this method, and use it with an increasing number of points to see how the precision improves.\n",
    "\n",
    "(curious about how this works in [infinitely many dimensions](https://davidegerosa.com/nsphere/)?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca366590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pi(npoints):\n",
    "  n_in_circle = 0\n",
    "  for i in range(npoints):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    if (x**2+y**2 < 1):\n",
    "      n_in_circle += 1\n",
    "  return 4*n_in_circle / npoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94f7565d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "c5cbdb5b-0465-4632-c280-1bd987f20afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6\n",
      "3.08\n",
      "3.1232\n",
      "3.142016\n"
     ]
    }
   ],
   "source": [
    "npoints = [10, 100, 10000, int(10e6)]\n",
    "for number in npoints:\n",
    "  print(pi(number))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56f297",
   "metadata": {},
   "source": [
    "As you can see, even with N=10 million points, the precision is not great. More specifically, the relative uncertainty on pi can be calculated as\n",
    "\n",
    "$$\\delta = 1/ \\sqrt{N}$$\n",
    "\n",
    "(If you don't know why go back to your stats textbook)\n",
    "\n",
    "Here is how the uncertainty evolves with the number of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f7908d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "_inYC2KBH4tc",
    "outputId": "f81c7d82-d16c-4731-d0b7-f9a6c3a9ddc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npoints 10 delta: 0.31622776601683794\n",
      "npoints 100 delta: 0.1\n",
      "npoints 10000 delta: 0.01\n",
      "npoints 10000000 delta: 0.00031622776601683794\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# defining the uncertainty function\n",
    "# with a lambda construct\n",
    "uncertainty = lambda x: 1/math.sqrt(x)\n",
    "for number in npoints:\n",
    "  print('npoints', number, 'delta:', uncertainty(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969b0cb",
   "metadata": {
    "id": "ddsXvcwmIv1t"
   },
   "source": [
    "Clearly, we'll need a lot of points. How fast is our code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07ed2b14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "K2coSx-YI5ol",
    "outputId": "51746db3-701f-4012-db6f-aebb8a401e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 s ± 2.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pi(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1edac",
   "metadata": {
    "id": "2GJsoGrFJLHK"
   },
   "source": [
    "A few seconds for 10 million points. This algorithm is O(N), so if we want to use 1 **billion** points, it will take us between 5 and 10 minutes . We don't have that much time, so let's use numba!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b63b6d58",
   "metadata": {
    "id": "8RNB6NbPJr9L"
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def fast_pi(npoints):\n",
    "  n_in_circle = 0\n",
    "  for i in range(npoints):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    if (x**2+y**2 < 1):\n",
    "      n_in_circle += 1\n",
    "  return 4*n_in_circle / npoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0e2ab2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Kmw3N-HgJytS",
    "outputId": "267a7d1c-bbc2-443c-c8ac-62368ada4c49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14152802"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_pi( int(1e9) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6223b4",
   "metadata": {
    "id": "GTd4sWAqMjrO"
   },
   "source": [
    "This took <10 s, instead of 10 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce22dc",
   "metadata": {
    "id": "oevjT7CP5KwW"
   },
   "source": [
    "## Finding the closest two points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b6722",
   "metadata": {
    "id": "EoQ10qzv8ug2"
   },
   "source": [
    "Numpy features an efficient implementation for most array operations. \n",
    "\n",
    "My suggestion: **If numpy can do it, just go for it.** Once you've abandoned your C mindeset of using indexes for everything, numpy is just easier to write, easier to read, the world is shining, and everything is beautiful again. \n",
    "\n",
    "But sometimes, you'll come up with an expensive algorithm that cannot easily be implemented with numpy. For instance, let's consider the following function, which takes an array of 2D points, and looks for the closest two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cdc1fbc",
   "metadata": {
    "id": "5iG7EBeK70YE"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def closest(points):\n",
    "  '''Find the two closest points in an array of points in 2D.\n",
    "  Returns the two points, and the distance between them'''\n",
    "\n",
    "  # we will search for the two points with a minimal\n",
    "  # square distance.\n",
    "  # we use the square distance instead of the distance\n",
    "  # to avoid a square root calculation for each pair of\n",
    "  # points\n",
    "\n",
    "  mindist2 = 999999.\n",
    "  mdp1, mdp2 = None, None\n",
    "  for i in range(len(points)):\n",
    "    p1 = points[i]\n",
    "    x1, y1 = p1\n",
    "    for j in range(i+1, len(points)):\n",
    "      p2 = points[j]\n",
    "      x2, y2 = p2\n",
    "      dist2 = (x1-x2)**2 + (y1-y2)**2\n",
    "      if dist2 < mindist2:\n",
    "        # squared distance is improved,\n",
    "        # keep it, as well as the two\n",
    "        # corresponding points\n",
    "        mindist2 = dist2\n",
    "        mdp1,mdp2 = p1,p2\n",
    "  return mdp1, mdp2, math.sqrt(mindist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f5a1d",
   "metadata": {
    "id": "qJjt9SPP2Eyg"
   },
   "source": [
    "You might be thinking that this algorithm is quite naive, and it's true! It was written like this on purpose.\n",
    "\n",
    "You can see that there is a double loop in this algorithm. So if we have N points, we would have to test NxN pairs of points, so the computational complexity here is $O(N^2)$.\n",
    "\n",
    "To improve the situation a bit, please note that the distance between point i and point j is the same as the distance between point j and point i!\n",
    "So there is no need to check this combination twice. Also, the distance between point i and itself is zero, and should not be tested...That's why we started the inner loop at i+1. So the combinations that are tested are:\n",
    "\n",
    "* (0,1), (0,2), ... (0, N)\n",
    "* (1,2), (1,3), ... (1, N)\n",
    "* ...\n",
    "\n",
    "Another thing to note is that we're doing all we can to limit the amount of computing power needed for each pair. That's why it's minimizing the square distance instead of the distance itself, which saves us a call to math.sqrt for every pair of points.\n",
    "\n",
    "Still, the algorithm remains $O(N^2)$.\n",
    "\n",
    "Let's first run this algorithm on a small sample of 10 points, just to check that it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d8d30da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "lkS5dbWv_TJA",
    "outputId": "c0b50a27-6f71-42b8-8ac5-83694925bd5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55545645  0.19532461]\n",
      " [ 0.93333211 -0.26740319]\n",
      " [ 0.21600159 -0.56871916]\n",
      " [ 0.79935586  0.26271225]\n",
      " [ 0.3757482  -0.93091315]\n",
      " [ 0.18387115  0.74700331]\n",
      " [-0.9426233  -0.13333073]\n",
      " [ 0.41215167  0.83672734]\n",
      " [ 0.56249722  0.31307642]\n",
      " [-0.3422276  -0.18140783]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.55545645, 0.19532461]),\n",
       " array([0.56249722, 0.31307642]),\n",
       " 0.11796211545817499)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = np.random.uniform((-1,-1), (1,1), (10,2))\n",
    "print(points)\n",
    "closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f6398",
   "metadata": {
    "id": "QF-j2ddw8vSH"
   },
   "source": [
    "Ok, this looks right, the two points indeed appear to be quite close. Let's see how fast is the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f5535c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gnOqlYxkAnPE",
    "outputId": "9a5f61f1-df9f-4631-d102-333a5e731241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.8 µs ± 173 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a522bf",
   "metadata": {
    "id": "v0Dp0UJZ2_-7"
   },
   "source": [
    "Now, let's increase a bit the number of points in the sample. You will see that the calculation will be much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a7c225c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "JJUyabLEApr0",
    "outputId": "636aff06-68e4-43af-9f45-9eaf1e757b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.84324155, 0.87772559]),\n",
       " array([0.84338028, 0.87746215]),\n",
       " 0.00029773565155952707)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = np.random.uniform((-1,-1), (1,1), (2000,2))\n",
    "closest(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c9231fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HkIm_2BgA2sM",
    "outputId": "ca54a6c7-beeb-450b-d92d-9fab8185d922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 s ± 8.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db9a10",
   "metadata": {
    "id": "ZOwo4aRj3Ic5"
   },
   "source": [
    "Since our algorithm is O(NxN), if we go from 10 to 2,000 points, the algorithm will be 200x200 = 40,000 times slower.\n",
    "\n",
    "Now let's try and speed this up with numba's just in time compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec950c3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "GSBhk2nlA4Zd",
    "outputId": "d07e4955-5295-4124-ec5d-1ab61cfcfada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.84324155, 0.87772559]),\n",
       " array([0.84338028, 0.87746215]),\n",
       " 0.00029773565155952707)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_closest = njit(closest) # This is equivalent to decorating with @\n",
    "c_closest(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2db4f10e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "5b5vaFXbEGCe",
    "outputId": "e4bcb82c-aebf-41df-9309-3d2e14c334cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.99333223,  0.2580123 ]),\n",
       " array([-0.99360315,  0.25866478]),\n",
       " 0.0007064888169048076)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_closest(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89a4477e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xaWTpO4dEIP1",
    "outputId": "506cf6b8-0638-4886-84c5-69ae7f555052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 s ± 6.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit closest(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0312484c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ohpaXF80LCE0",
    "outputId": "b2daf05f-e968-4107-c191-21e715316c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.82 ms ± 55.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit c_closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bff98c",
   "metadata": {
    "id": "Z4uXBrNe_Kv5"
   },
   "source": [
    "Again, the compiled code is 100 times faster! And this time we don't have a numpy alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353d720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd640b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf9c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09314d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc968ea4",
   "metadata": {},
   "source": [
    "# Part 2. Parallel computing: multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c70bf",
   "metadata": {},
   "source": [
    "Why running code in parallel? To go faster.\n",
    "\n",
    "Ultimately the number of operations one can do is set by the chip.\n",
    "\n",
    "\n",
    "That said, things are flattening out (\"Moore law is dead\") and powerful chips are expensive.\n",
    "\n",
    "Two modern solutions:\n",
    "\n",
    "1. Break down your computation and use more chips. That's **parallel computing**.  Two ways, depending on your problem.\n",
    "    - HPC (High Performance Computing). That's the real parallelization business, where you break down your computation into chunks, send the chunks to different processors, and (crucially) these processors need to talk to each other. For instance, solving a complicated PDE using subdomains, you need to exchange information on the boundary conditions. \n",
    "    - HTC (High Throughput Computing), also called \"embarrassing\" parallelization. Here the various jobs don't need to talk to each other. For instance a parameter-space study. You need to perform a costly operation many times, and all of these are independent of each other. \n",
    "    - .\n",
    "    \n",
    "2. Use **different hardware**. We're not going to go into this but you might have heard of things like:\n",
    "    - GPU. That's the thing that renders your screen (the market is indeed dominated by gaming!), which it turns our it's very good at dealing with linear algebra. Deep learning applications are essentially a pile of matrices.\n",
    "    - TPU. That's Google's hardware which is highly optimized to run Google's TensorFlow pipelines.\n",
    "    - FPGA. Programmable hardware. I don't know anything about this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef96683",
   "metadata": {},
   "source": [
    "Let's look at **embarrassingly parallel programming** with python, hope this is useful for some of your research needs.\n",
    "\n",
    "For true HPC in python, look into things like [mpi4py](https://mpi4py.readthedocs.io/en/stable/)\n",
    "\n",
    "One can parallelize things using multiple cores of a single machine, or using multiple machines.\n",
    " - Single-node parallelization means using all the cores in a chip (can be useful even on your laptop!)\n",
    " - Multi-node parallelization, well, requires multiple computers that are appropriately linked together. Say in a big cluster (is any of you using Galileo or Leonardo at CINECA?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57876e",
   "metadata": {},
   "source": [
    "In python, embarassingle parallelization on a single node is done with the **multiprocessing** package. Here we use a wrapper provided in the 3rd-party **pathos** package. It's the same, just a bit more user fiendly IMO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30b3487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You might need to pip install pathos and tqdm\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing, pathos.multiprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef110004",
   "metadata": {},
   "source": [
    "I want to execute a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4e5b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a,b):\n",
    "    #print(os.getpid())\n",
    "    time.sleep(1) # So that it takes a bit longer...\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757a39c",
   "metadata": {},
   "source": [
    "... many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "314061b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "a=np.linspace(0,1,N)\n",
    "b=np.linspace(0,1,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76a51979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       " array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d374b1",
   "metadata": {},
   "source": [
    "I can do it with a for loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34900b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 ms, sys: 1.31 ms, total: 3.32 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aplusb=[]\n",
    "for ax,bx in zip(a,b):\n",
    "    aplusb.append(fun(ax,bx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "362d8c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.2222222222222222,\n",
       " 0.4444444444444444,\n",
       " 0.6666666666666666,\n",
       " 0.8888888888888888,\n",
       " 1.1111111111111112,\n",
       " 1.3333333333333333,\n",
       " 1.5555555555555554,\n",
       " 1.7777777777777777,\n",
       " 2.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aplusb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7007e",
   "metadata": {},
   "source": [
    "(of course this is a stupid example and you should do it with numpy! But let's say `fun` is a complicate thing of thousands of lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c9866",
   "metadata": {},
   "source": [
    "We can implement the same thing using python's map function. This is not going to be any faster, it's just a compact way of writing a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b8eeb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 ms, sys: 1.9 ms, total: 3.79 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "aplusb = list(  map(fun, a,b)  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adac50f",
   "metadata": {},
   "source": [
    "... do it again, just adding a nice progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b132a2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd218d28645147aaa0ed03f5c161b4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.2222222222222222, 0.4444444444444444, 0.6666666666666666, 0.8888888888888888, 1.1111111111111112, 1.3333333333333333, 1.5555555555555554, 1.7777777777777777, 2.0]\n",
      "CPU times: user 32 ms, sys: 12.9 ms, total: 45 ms\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "aplusb = list(tqdm(map(fun, a,b),total=N))\n",
    "print(aplusb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a1d82",
   "metadata": {},
   "source": [
    "Now let's try to make full use of my computer. How many cores do I have on my chip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e148320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddf49c",
   "metadata": {},
   "source": [
    "python's map syntax can be easily generalized to use multiple cores. Let's use a few of them, all at once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88ccb3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be338d0edbf742728d282c423cd0937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 ms, sys: 8.26 ms, total: 34.9 ms\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "CPUS = 4\n",
    "\n",
    "# Now, this is much faster!\n",
    "parmap = pathos.multiprocessing.ProcessingPool(CPUS).imap\n",
    "\n",
    "\n",
    "aplusb = list(tqdm(parmap(fun, a,b),total=N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cba19d",
   "metadata": {},
   "source": [
    "That's faster! And with minimal editing to the code (just define parmap and change map --> parmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18acb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplusb = list( tqdm( map(    fun, a,b), total=N))\n",
    "# aplusb = list( tqdm( parmap( fun, a,b), total=N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78222b4d",
   "metadata": {},
   "source": [
    "Note the scaling is not perfectly linear, because there's an overhead when spawning new processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40807b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bbc2e95",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "For the exam prepare two of these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d36c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q1: The stock market\n",
    "\n",
    "(This is about numba)\n",
    "\n",
    "A Markov Chain is defined as a sequence of random variables where a parameter depends *only* on the preceding value. This is a crucial tool in statistics, widely used in science and beyond (economics for instance).\n",
    "\n",
    "For instance, the stock market has phases of growing prices (bull), dreasing prices (bear) and recession. This would be a Markov Chain model:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Finance_Markov_chain_example_state_space.svg/400px-Finance_Markov_chain_example_state_space.svg.png)\n",
    "\n",
    "where the numbers on the arrows indicate the probability that the next day will be in a given state.\n",
    "\n",
    "Your task is to simulate the stock market according to this rule. Start from a random state and simulate many many  iterations. If your code is right, the fraction of days in each state should converge. \n",
    "\n",
    "Implement a pure-python version and a numba version, and compare speeds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b16abc",
   "metadata": {},
   "source": [
    "## Q2: Consistent plotting\n",
    "\n",
    "(This is about python's dectorators)\n",
    "\n",
    "Write a decorator for the plots of all your papers. \n",
    "\n",
    "- Remember a decorator takes a function. \n",
    "- This function in turn should return a matplotlib figure object.\n",
    "- Before the function is called, the decorator should initialize a matplotlib figure with the options that you like the most (fontsize, ticks, etc etc)\n",
    "- After the figure it's done, the decorator should save it to pdf.\n",
    "\n",
    "This is a great hack for your papers! You do this once and for all, and all plots in your paper will be beautifull, all with the same style/fontsize/etc. All you'll need to do is adding `@myplot` to the relevant plotting functions. \n",
    "    \n",
    "The decorator that I use for my papers is available in my python module [skywalker](https://github.com/dgerosa/skywalker).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1b65e",
   "metadata": {},
   "source": [
    "## Q3: Scaling\n",
    "\n",
    "(This is about multiprocessing)\n",
    "\n",
    "The [\"scaling\"](https://hpc-wiki.info/hpc/Scaling) of a code refers to its performance of as a function of the number of cores adopted. \n",
    "\n",
    "- Define a computationally intensive task (something like an operation on two giant arrays with >1e7 numbers or, even better!, pick somethinbg from your research). \n",
    "- Make sure it's embarrassingly parallel. \n",
    "- Implement a parallelization strategy using multiprocessing. \n",
    "- Plot the time the code takes as a function of the number of cores.\n",
    "- Figure out the number of cores in your CPU and make sure the plot extends both below and above this number.\n",
    "- Interpret the resulting features. \n",
    "- A perfect scaling results in straight line (linear dependency). How perfect is your scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c5c11",
   "metadata": {},
   "source": [
    "### Important\n",
    "Numpy has some inner, semi-automatic parallelization functionalities. Some, but not all, numpy functions detect the number of CPUs in your machine and make good use of them. That's great for most applications, but when performing a scaling study you want to control the parallelization yourself and disable what's done by numpy's. The following forces numpy to use a single core.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9acfd0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openblas64__info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "blas_ilp64_opt_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "openblas64__lapack_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "lapack_ilp64_opt_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "Supported SIMD extensions in this NumPy install:\n",
      "    baseline = NEON,NEON_FP16,NEON_VFPV4,ASIMD\n",
      "    found = ASIMDHP,ASIMDDP\n",
      "    not found = ASIMDFHM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__config__.show()\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57743a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3adb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "numba_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
